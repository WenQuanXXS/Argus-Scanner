#  Argus-Scanner 漏洞检测性能统一测试方案

## 1. 测试目标
建立一套标准化、可重复的自动化性能测试流程，以评估系统在不同语言和攻击场景下的检测能力。本方案以 **降低漏报率 (False Negative Rate, FNR)** 为最高优先级，同时兼顾误报率 (False Positive Rate, FPR) 的控制。

## 2. 核心基准测试集 (The Benchmark Suite)
为保证测试的全面性与权威性，选用以下四个开源项目作为标准测试集：

| 编号 | 项目名称 | 语言 | 测试重点 | 获取方式 |
| :--- | :--- | :--- | :--- | :--- |
| **B1** | **Juliet Test Suite** (v1.3 subset) | C/C++/Java | **综合指标** (NIST标准)，覆盖 100+ CWE 类型。 | `test_references/Juliet` |
| **B2** | **Noriben** | Python | **动态行为捕获**。测试沙箱对进程创建、文件操作的监控能力。 | `test_references/Noriben` |
| **B3** | **Open-Audit** | Java/Go | **规则匹配与污点分析**。测试对敏感 Sink 点（如 `exec`）的识别。 | `test_references/Open-Audit` |
| **B4** | **Expat** (Libexpat) | C | **内存安全检测**。测试对已知 CVE（缓冲区溢出等）的静态分析能力。 | `test_references/libexpat` |

## 3. 测试环境与目录结构
所有测试资源统一存放于项目根目录下的 `test_references/` 文件夹中。

```text
Argus-Scanner/
├── test_references/          # [新建] 存放基准测试项目的源码
│   ├── Juliet/               # NIST Juliet Test Suite (或其子集)
│   ├── Noriben/              # Python 恶意行为模拟
│   ├── Open-Audit/           # Java/Go 审计工具
│   └── libexpat/             # C 语言库 (含历史漏洞)
├── tests/
│   └── performance/
│       ├── run_benchmarks.py    # [核心] 统一测试启动脚本
│       ├── benchmark_config.yaml # [核心] 测试用例与预期结果配置
│       └── ground_truth.json    # 详细的漏洞基准数据
└── ...
```

## 4. 自动化测试流程
测试通过 `tests/performance/run_benchmarks.py` 脚本统一执行。

### 步骤 1：环境准备
脚本首先检查 `test_references/` 下各项目是否存在，若缺失则尝试自动克隆或提示用户下载。

### 步骤 2：执行扫描
依次对每个基准项目调用 `main.py` 进行全量扫描。目标路径作为位置参数传递。
- **命令示例**: `python main.py test_references/Noriben --format all --output tests/performance/reports`
- **注意**: 自动化脚本默认使用 `--format all`，会同时生成 JSON（用于自动比对）和 HTML（用于人工查看）格式的报告。

### 步骤 3：结果比对
解析生成的 JSON 报告，与 `benchmark_config.yaml` 或 `ground_truth.json` 中的预期结果进行比对。

### 步骤 4：指标计算
- **漏报率 (FNR)** = (已知的未检出漏洞数 / 已知漏洞总数) * 100%
- **通过标准**: 核心高危漏洞（如 RCE, SQLi）漏报率需 < **5%**（示例值，可调整）。

## 5. 配置文件说明 (benchmark_config.yaml)
```yaml
benchmarks:
  noriben:
    path: "test_references/Noriben"
    language: "python"
    expected_findings:
      - type: "Suspicious System Call"
        min_count: 2
      - type: "Network Activity"
        min_count: 1
  libexpat:
    path: "test_references/libexpat"
    language: "c"
    expected_findings:
      - type: "Buffer Overflow"
        min_count: 5
```

## 6. 执行测试
在项目根目录下运行：
```bash
python tests/performance/run_benchmarks.py
```
若需测试特定项目：
```bash
python tests/performance/run_benchmarks.py --target noriben
```

## 7. 生成可视化报告 (HTML)
虽然自动化脚本默认生成 JSON 以供比对，但您也可以手动生成直观的 HTML 报告：

### 方法 A：通过脚本指定格式
修改 `run_benchmarks.py` 中的扫描参数，或直接运行以下命令：
```bash
python main.py test_references/Noriben --format html --output reports/
```

### 方法 B：一次生成所有格式
```bash
python main.py <目标路径> --format all --output reports/
```
扫描完成后，在 `reports/` 目录下会生成一个交互式的 HTML 文件，包含：
- 漏洞风险等级统计图表。
- 详细的代码片断及污点追踪路径。
- 可交互的过滤器（按严重程度、类型过滤）。